{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Landsat Satellite Classification\n",
    "## Project Purpose\n",
    "Landsat data is one of the many sources of information available on site. With the advent of the era characterized by remote sensing integrated methods, it will be of great significance to interpret scenes by integrating different types and resolutions of spatial data (including multi-spectral and radar data, maps showing terrain, land use, etc.) (for example, NASA Earth Observation System since the beginning of this decade). The existing statistical methods are not suitable for dealing with such different data types. This project aims to predict this classification based on neural networks, given the multi-spectral values. \n",
    "\n",
    "## Problem Statement\n",
    "The database comprises the multi-spectral values of the pixels in 3x3 neighbors in a satellite picture and the categorization linked to each neighborhood's center pixel. With the help of the multi-spectral readings, this categorization will be predicted. The category of a pixel is encoded as an integer in the dataset.\n",
    "\n",
    "## Dataset Description\n",
    "The dataset I use is the Landsat Satellite Dataset. It was generated from Landsat Multi-Spectral Scanner image data, which the Department of Statistics and Modelling Science, University of Strathclyde, provided. In the sample database, the class of a pixel is coded as an integer, ranging from 0 to 255. This dataset consists of 4435 training samples and 2000 testing samples. All the samples have 36 features, four spectra in each sample, and every spectral is 3x3 neighbors in a satellite image. The label of this sample is also an integer representing a specific class. The meaning of labels is shown in the table below:\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>label</th>\n",
    "      <th>class</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>red soil</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>cotton crop</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>grey soil</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>damp grey soil</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>5</td>\n",
    "      <td>soil with vegetation stubble</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7</td>\n",
    "      <td>very damp grey soil</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "## Project Repository\n",
    "https://github.com/yichiz3/project-7300\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21midx2label\u001b[39m(idx):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\ml\\project-7300\\util.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "\n",
    "def idx2label(idx):\n",
    "    if idx<5:\n",
    "        return idx+1\n",
    "    return idx+2\n",
    "\n",
    "def label2idx(label):\n",
    "    if label<=5:\n",
    "        return label-1\n",
    "    return 5\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.x = np.array(df.iloc[:, :-1]) / 255\n",
    "        self.x = torch.tensor(self.x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(df.iloc[:, -1].map(label2idx), dtype=torch.int64)\n",
    "\n",
    "    def __getitem__(self, index):     \n",
    "        return self.x[index, :], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.x.shape[0])\n",
    "\n",
    "\n",
    "def get_data(batch_size=32):\n",
    "    train_data = pd.read_csv(PROJ_PATH+\"/sat.trn\", sep=' ', header=None)\n",
    "    test_data = pd.read_csv(PROJ_PATH+\"./sat.tst\", sep=' ', header=None)\n",
    "    train_set = Dataset(train_data)\n",
    "    test_set = Dataset(test_data)\n",
    "    train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing data are stored in the file ‘sat.trn’ and ‘sat.tst’, respectively. First, training and testing samples are read into the pandas data frame format. The features of each sample are divided by 255 and transformed into a float number in (0, 1). Then, the features and labels of the dataset will be turned into pytorch tensors and proceed as pytorch dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "### The Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = pd.read_csv(PROJ_PATH+\"/sat.trn\", sep=' ', header=None)\n",
    "test_data = pd.read_csv(PROJ_PATH+\"./sat.tst\", sep=' ', header=None)\n",
    "train_label = train_data.iloc[:, -1]\n",
    "test_label = test_data.iloc[:, -1]\n",
    "\n",
    "train_map = {}\n",
    "for label in train_label.values:\n",
    "    if label not in train_map:\n",
    "        train_map[label] = 1\n",
    "    else:\n",
    "        train_map[label] += 1\n",
    "sum_ = sum(train_map.values())\n",
    "\n",
    "def pie():\n",
    "    sum_ = sum(train_map.values())\n",
    "\n",
    "    plt.pie(np.array(list(train_map.values()))/sum_, labels=train_map.keys(),autopct='%.1f%%')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    test_map = {}\n",
    "    for label in test_label.values:\n",
    "        if label not in test_map:\n",
    "            test_map[label] = 1\n",
    "        else:\n",
    "            test_map[label] += 1\n",
    "    sum_ = sum(test_map.values())\n",
    "    \n",
    "    plt.pie(np.array(list(test_map.values()))/sum_, labels=test_map.keys(),autopct='%.1f%%')\n",
    "    plt.show()  \n",
    "\n",
    "pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see that, the distribution of labels is roughly even. No label has occupied most of the proportion or which is too less for the model to learn. The distribution is similar on the training set compared to the label distribution on the testing set. Label 7(very damp grey soil) is the label with the most significant number of samples, with a proportion of 23.4-23.5%. Label 4(damp grey soil) has the lowest proportions, around 9.4%-10.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Histogram of the pixel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hist():\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.hist(train_data.iloc[:, 0].values)\n",
    "    plt.title(\"Histogram of Pixel Values on the 1st dimension\")\n",
    "    plt.show()\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.hist(train_data.iloc[:, 9].values)\n",
    "    plt.title(\"Histogram of Pixel Values on the 10th dimension\")\n",
    "    plt.show()\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.hist(train_data.iloc[:, 18].values)\n",
    "    plt.title(\"Histogram of Pixel Values on the 19th dimension\")\n",
    "    plt.show()\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.hist(train_data.iloc[:, 27].values)\n",
    "    plt.title(\"Histogram of Pixel Values on the 28th dimension\")\n",
    "    plt.show()\n",
    "hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have done predictive variable analysis. There are totally 36 variables, so we pick up the 1st, 10th, 19th and 28th variable to draw the histogram of pixel values. \n",
    "The distribution of the 1st, 10th and 19th variable is approximately symmetric. Most of the pixel values is gathered around the center of the distribution. The distribution for the 28th pixel value if right skewed, most of the variables is gathered at the smaller values of the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correlation():\n",
    "    corrdata=train_data.iloc[:, [0, 9, 18, 27]]\n",
    "    corr = corrdata.corr()\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.heatmap(corr, annot=True, annot_kws={\"size\": 15})\n",
    "    \n",
    "    plt.title(\"Correlation between four spectrums\")\n",
    "    plt.show()\n",
    "correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from Figure 2, there is a high correlation between spectral bands 3 and 4. However, there is no correlation between others. We can assume that spectral bands 3 and 4 are likely to have similar values simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details\n",
    "Considering that all the features of the observations of the dataset are continuous numeric variables, I choose to apply logistic regression and Neural Network to solve the classification issue in this report because the parameter of these two models is naturally continuous. The structure of logistic regression is simple but effective. Generally, logic regression and neural networks with SoftMax are used to find the optimal linear separable boundaries. The feature dimension here is 36, as the higher the dimension is, the more likely we might make it possible to draw a linear separable boundary for the samples. Then, I applied decision tree and adaboost to test their performance.\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic(nn.Module):\n",
    "    \"\"\"\n",
    "    Logistic Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=36, output_dim=6):\n",
    "        super(Logistic, self).__init__()\n",
    "        self.line = nn.Linear(input_dim, output_dim)\n",
    "        self.activate = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.line(x)\n",
    "        return self.activate(h1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dimension of the logistic regression and the output dimension is 6, with each dimension representing the probability for the data to be a specific class. There are no hyper-parameters for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Two Layer NN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=36, hidden_dim=72, output_dim=6):\n",
    "        super(Model, self).__init__()\n",
    "        self.line1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.line2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activate = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.line1(x)\n",
    "        a1 = torch.relu(h1)\n",
    "        return self.activate(self.line2(a1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the model of the two-layer neural network, we can choose to set up the output dimension of the first linear layer and activate this layer's function. After several tries, I set the first layer's hidden dimension to 72 and applied a relu function as the activate function. The output dimension is 6, which is equal to the number of labels. The structure of my two-layer neural network model is shown below. ![Model Structuer](C:\\Users\\gqf12\\Documents\\ml\\project-7300\\nn.png)\n",
    "\n",
    "I use batch gradient descent Algorithms for updating the parameters in each epoch. For each batch, the output of logistic regression can be formatted as:\n",
    "$$h_1=xW∈R^{Batch×6}$$\n",
    "$$y_score=softmax(h_1)∈R^{Batch×6}$$\n",
    "where $W∈R^{36×6}$ is the linear metrics. \n",
    "\n",
    "Moreover, the softmax function is known as:\n",
    "$$softmax(x_i)=e^{x_i}/(∑e^{x_j} )$$\n",
    "it can convert the predicted score as predicted probabilities.\n",
    "As for the two-layer neural networks, the formula is a bit of complicated. Suppose the input of the model is $x∈R^{Batch×36}$, then in the first layer:\n",
    "$h_1=xW_1∈R^{Batch×72}$\n",
    "where $W_1∈R^{36×72}$ is the linear metrics in layer 1. Then $h_1$ is passes through relu activation:\n",
    "$$a_1=relu(h_1)∈R^{Batch×72}$$\n",
    "The output of the first layer $a_1$, will be send into the second layer:\n",
    "$$h_2=a_1 W_2∈R^{Batch×6}$$\n",
    "where $W_2∈R^{72×6}$ is the linear metrics in layer 2. Finally, the after softmax function, the output probability for this batch, y_score, is obtained by:\n",
    "$$y_{score}=softmax(h_2)∈R^{Batch×6}$$\n",
    "The cross entropy loss function:\n",
    "$$CrossEntropy(y,y)=∑_i y_i log(\\hat{y_i})$$\n",
    " is chosen as the loss function. And ADAM with learning rate equals to 0.005 is chosen as the optimizer of the network. In practical the batch size is set to be 64. The model is trained in 15 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, \n",
    "                 right=None, info_gain=None, value=None):\n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        # for leaf node\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        ''' recursive function to build the tree ''' \n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        ''' function to compute information gain '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        ''' function to compute entropy '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        ''' function to compute gini index '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict new dataset '''\n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "\n",
    "    def predict_One(self, single_X):\n",
    "        return self.make_prediction(single_X, self.root) \n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict a single data point '''\n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Adaboost:\n",
    "    def __init__(self, num_learner: int, Model, random_State=None, \n",
    "                 min_samples_split_lower=2, min_samples_split_upper=5, max_depth_lower=2, max_depth_upper=6):\n",
    "        self.num_learner = num_learner\n",
    "        self.entry_weights = None\n",
    "        self.learner_weights = None\n",
    "        self.sorted_learners = None\n",
    "        self.map_Label_2_Index = {}\n",
    "        self.learners = self.create_Learners(\n",
    "            Model=Model,\n",
    "            random_State=random_State,\n",
    "            min_samples_split_lower=min_samples_split_lower,\n",
    "            min_samples_split_upper=min_samples_split_upper,\n",
    "            max_depth_lower=max_depth_lower,\n",
    "            max_depth_upper=max_depth_upper)\n",
    "\n",
    "    def create_Learners(self, Model, random_State, min_samples_split_lower=2, min_samples_split_upper=5, \n",
    "                        max_depth_lower=2, max_depth_upper=6):\n",
    "        if random_State:\n",
    "            random.setstate(random_State)\n",
    "        learners = []\n",
    "        for i in range(self.num_learner):\n",
    "            learners.append(\n",
    "                Model(\n",
    "                    min_samples_split=random.randint(\n",
    "                        min_samples_split_lower,\n",
    "                        min_samples_split_upper),\n",
    "                    max_depth=random.randint(\n",
    "                        max_depth_lower,\n",
    "                        max_depth_upper\n",
    "                    )))\n",
    "        return learners\n",
    "\n",
    "    def __fit_Learners(self, X_Train, Y_Train):\n",
    "        for learner in self.learners:\n",
    "            learner.fit(X_Train, Y_Train)\n",
    "\n",
    "    def fit(self, X_Train, Y_Train):\n",
    "        self.num_Of_Classes = len(np.unique(Y_Train))\n",
    "        self.map_Label_2_Index = {\n",
    "            label: index for index, label in enumerate(np.unique(Y_Train))}\n",
    "        self.map_Index_2_Label = {\n",
    "            index: label for index, label in enumerate(np.unique(Y_Train))}\n",
    "\n",
    "        num_Of_Data = X_Train.shape[0]\n",
    "        self.__fit_Learners(X_Train=X_Train, Y_Train=Y_Train)\n",
    "        self.entry_weights = np.full(\n",
    "            (num_Of_Data,), fill_value=1/num_Of_Data, dtype=np.float32)\n",
    "        self.learner_weights = np.zeros((self.num_learner,), dtype=np.float32)\n",
    "\n",
    "        score = [0 for i in range(self.num_learner)]\n",
    "        for learner_idx, learner in enumerate(self.learners):\n",
    "            score[learner_idx] = accuracy_score(\n",
    "                learner.predict(X_Train), Y_Train)\n",
    "        self.sorted_learners = [l for l, e in sorted(\n",
    "            zip(self.learners, score), key=lambda pair: pair[1], reverse=True)]\n",
    "\n",
    "        for learner_idx, learner in enumerate(self.sorted_learners):\n",
    "            Y_Predicted = learner.predict(X_Train)\n",
    "            is_wrong = np.array(Y_Predicted != Y_Train.reshape(-1)).astype(int)\n",
    "            weighted_learner_error = np.sum(\n",
    "                is_wrong * self.entry_weights)/self.entry_weights.sum()\n",
    "            self.learner_weights[learner_idx] = max(0, \n",
    "                    np.log(1/(weighted_learner_error + 1e-6) - 1) + np.log(\n",
    "                self.num_Of_Classes - 1))\n",
    "            alpha_arr = np.full(\n",
    "                (num_Of_Data,), fill_value=self.learner_weights[learner_idx], \n",
    "                                                dtype=np.float32)\n",
    "            self.entry_weights = self.entry_weights * \\\n",
    "                np.exp(alpha_arr * is_wrong)\n",
    "            self.entry_weights = self.entry_weights/self.entry_weights.sum()\n",
    "\n",
    "        self.learner_weights = self.learner_weights/self.learner_weights.sum()\n",
    "\n",
    "    def predict(self, features):\n",
    "        return [self.predict_One(feature) for feature in features]\n",
    "\n",
    "    def predict_One(self, feature):\n",
    "        pooled_prediction = np.zeros((self.num_Of_Classes,), dtype=np.float32)\n",
    "        for learner_idx, learner in enumerate(self.sorted_learners):\n",
    "            predicted_cat = learner.predict_One(feature)\n",
    "            prediction = np.full(\n",
    "                (self.num_Of_Classes,), fill_value=-1/(self.num_Of_Classes-1), \n",
    "                                                        dtype=np.float32)\n",
    "            prediction[self.map_Label_2_Index[predicted_cat]] = 1\n",
    "            pooled_prediction += prediction*self.learner_weights[learner_idx]\n",
    "\n",
    "        return self.map_Index_2_Label[np.argmax(pooled_prediction)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_or_eval(model, epochs=15, batch_size=32, lr=5e-3, l2=1e-2):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_loader, test_loader = get_data(batch_size)\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        y_preds = []\n",
    "        y_truth = []\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            y_score = model(x)\n",
    "            y_pred = y_score.argmax(-1)\n",
    "            y_pred = y_pred\n",
    "            y_preds.extend(y_pred.numpy().tolist())\n",
    "            y_truth.extend(y.numpy().tolist())\n",
    "            loss = loss_func(y_score, y)\n",
    "            total_loss += loss.item() * y_pred.shape[0]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        print(\"training details for epoch \", e+1)\n",
    "        losses.append(total_loss / len(y_preds))\n",
    "        print(\"loss: \", losses[-1])\n",
    "        print(classification_report(y_truth, y_preds, target_names=['1','2','3','4','5','7']))\n",
    "        print(confusion_matrix(y_truth, y_preds))\n",
    "        \n",
    "        model.eval()\n",
    "        y_preds = []\n",
    "        y_truth = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                y_score = model(x)\n",
    "                y_pred = y_score.argmax(-1)\n",
    "                y_pred = y_pred\n",
    "                y_preds.extend(y_pred.numpy().tolist())\n",
    "                y_truth.extend(y.numpy().tolist()) \n",
    "        print(\"testing details for epoch \", e+1)\n",
    "        print(classification_report(y_truth, y_preds, target_names=['1','2','3','4','5','7']))\n",
    "        print(confusion_matrix(y_truth, y_preds))\n",
    "    return losses     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = Logistic()\n",
    "lr_losses = train_or_eval(lr_model, epochs=30, batch_size=64, lr=1e-2)\n",
    "plt.plot(lr_losses)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, among the 6 classes, the model has the highest precision 0.78 on class 3(grey soil) and highest recall 0.99 on class 1(red soil), and the model perform poor on class 2(cotton crop), class 4(damp grey soil) and class 5(soil with vegetation stubble). The overall accuracy on the testing set is 0.64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Model()\n",
    "nn_losses = train_or_eval(nn_model, epochs=15, batch_size=64)\n",
    "plt.plot(nn_losses)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the 6 classes, the model has the highest precision 0.93 and highest recall 0.99 on class 1(red soil), and the model perform poor on class 4(damp gray soil), none of the samples from class 4 is classified correctly. The overall accuracy on the testing set is 0.82. Also, the loss curve shows that the training loss for the neural network gets stable after 11 epochs. As for the logistic regression, it was not until 30 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from StandardScaler import StandardScaler\n",
    "train_data = pd.read_csv(\"sat.trn\", sep=' ', header=None)\n",
    "test_data = pd.read_csv(\"sat.tst\", sep=' ', header=None)\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(np.array(train_data.iloc[:, :-1]) )\n",
    "y_train = np.array(list(map(label2idx, train_data.iloc[:, -1])))\n",
    "x_test = sc.transform(np.array(test_data.iloc[:, :-1]) )\n",
    "y_test = np.array(list(map(label2idx, test_data.iloc[:, -1])))\n",
    "\n",
    "model_Tree = DecisionTreeClassifier(min_samples_split=4, max_depth=5)\n",
    "model_Tree.fit(x_train, y_train.reshape([-1,1]))\n",
    "model_Tree.print_tree()\n",
    "Y_Pred_Tree = model_Tree.predict(x_train)\n",
    "print(\n",
    "    f\"Decision Tree Classifier accruacy for training is {accuracy_score(y_train, Y_Pred_Tree)}\")\n",
    "print(\"Confusion matrix for training:\")\n",
    "print(confusion_matrix(y_train, Y_Pred_Tree))\n",
    "\n",
    "\n",
    "pred_Test_Tree = model_Tree.predict(x_test)\n",
    "print(\n",
    "    f\"Test accuracy score of dicision tree is: {accuracy_score(pred_Test_Tree, y_test)}\")\n",
    "print(confusion_matrix(y_test, pred_Test_Tree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also used decision tree classifier as comparison. The max depth hyperparameter is set to be 5 and minimum samples of a node is 4 to have low bias and variance. The decision tree model has test accuracy score of 0.84 which is similiar to the performance of logistic regression and neural network. But from the diagnal of the confusion matrix which is the true positive value of the predicted class, decision tree will separate all the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Adabost = Adaboost(num_learner=10, Model=DecisionTreeClassifier, min_samples_split_lower=3,\n",
    "                         min_samples_split_upper=5, max_depth_lower=4, max_depth_upper=6)\n",
    "model_Adabost.fit(x_train, y_train.reshape(-1, 1))\n",
    "Y_Pred_ada = model_Adabost.predict(x_train)\n",
    "print(\n",
    "    f\"Adaboost accuracy is {accuracy_score(Y_Pred_ada, y_train.reshape(-1))}\")\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_train, Y_Pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also used Adaboost algorithm with Decision tree as conjunction. The model has 10 decision tree learners with minimum sample split of 3 and maximum of 5, and depth ranging from 4 to 6. Adaboost uses multiple weak decision tree models to prevent overfitting. As we can see from the result, Adaboost has better accuracy while be overfitting. Adaboost has accuracy of 0.85 which is higher than decision tree, logistic regression and neural network in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "Based on the loss curves shown in Figures 5 and 6, when the loss gets stable, the training loss for the two-layer neural network is smaller than the training loss of logistic regression. Moreover, the loss curve's convergence speed for the neural network is also faster than the logistic regression.\n",
    "Taking the classification accuracy, the overall accuracy for the testing set of logistic regression is only 0.64. As for the neural network, this number can come up to 0.82. Seen from the classification condition within and across the class, the performance of logistic regression is poor in classes 2, 4, and 5. In contrast, the neural network's performance is only poor in class 4. Also, the precision and recall rate on other classes tell us that the neural network better classifies the samples. I select the two-layer neural network as the final classification model.\n",
    "\n",
    "## Conclusion and Recommendation\n",
    "In this report, I choose the Landsat Satellite dataset. The features of the samples in this dataset are all pixel values. I did an exploratory analysis on the distribution of pixel values of several features and checked if the distribution of labels was almost the same for the training and testing set. After that, I applied logistic regression and neural network as two candidate classification models and selected neural network as the final model for classifying the Landsat Satellite dataset based on their loss curves and classification reports.\n",
    "After selecting the final model, according to the result of the confusion matrix of the model, we can see that, among the 211 samples from class 4, 76 out of them are misclassified into class 3 (grey soil), and 123 out of them are misclassified into class 7(very damp grey soil). Those two classes might be similar to the damp grey soil, so it is difficult for this model to recognize.\n",
    "I recommend that this neural network model be used for Landsat Satellite classification for all soil classes except for damp gray soil. It performs well, especially on red soil and cotton crops.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f0961e0ab96e2cd5704b29a2d9af1c5c54a390cd1ac30ac1db124df242579397"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
